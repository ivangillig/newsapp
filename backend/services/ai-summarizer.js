import OpenAI from 'openai'
import { logger } from '../utils/logger.js'
import NewsCache from '../models/NewsCache.js'
import { scrapeAllPortals, scrapeArticle } from './scraper.js'

// Lazy initialization - created when used, not on import
let openai = null

function getOpenAI() {
  if (!openai) {
    openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    })
  }
  return openai
}

const SYSTEM_PROMPT = `Sos un analista de noticias experto argentino.
Te paso una lista de art√≠culos scrapeados de portales. Tu tarea es seleccionar y categorizar los m√°s importantes.

ESTRUCTURA JSON REQUERIDA:
{
  "categories": [
    {
      "name": "PRINCIPALES",
      "urls": ["https://...", "https://...", ...]
    },
    {
      "name": "POL√çTICA",
      "urls": ["https://...", ...]
    }
    // ... dem√°s categor√≠as
  ]
}

REGLAS ESTRICTAS:

1. CATEGOR√çAS DISPONIBLES (en este orden):
   - PRINCIPALES (exactamente 5 URLs - las noticias m√°s relevantes del d√≠a, solo para WhatsApp)
   - POL√çTICA
   - ECONOM√çA
   - MUNDO
   - CIENCIA Y TECNOLOG√çA
   - SOCIEDAD
   - DEPORTES
   - ESPECT√ÅCULOS
   - SEGURIDAD Y DEFENSA (incluye: crimen, ataques militares, defensa nacional, violencia, robos, narcotr√°fico, etc.)
   - CLIMA

2. SELECCI√ìN POR CATEGOR√çA (MUY IMPORTANTE):
   - PRINCIPALES: Exactamente 5 URLs (las m√°s importantes del d√≠a, solo para WhatsApp)
   - TODAS las dem√°s categor√≠as: M√çNIMO 3 URLs, M√ÅXIMO 4 URLs cada una
   - Si una categor√≠a no tiene 3 noticias relevantes, buscar m√°s relacionadas
   - OBJETIVO TOTAL: Aproximadamente 30 o 35 art√≠culos (sin contar PRINCIPALES)
   - Las URLs de PRINCIPALES S√ç DEBEN repetirse en sus categor√≠as correspondientes
   - Solo seleccion√° URLs que realmente existan en la lista que te paso
   - S√â AGRESIVO: Inclu√≠ todas las noticias interesantes, no seas conservador
   - Ejemplo: Si hay noticias de F1, f√∫tbol, tenis ‚Üí todas van a DEPORTES hasta completar 4
   - Ejemplo: Ataques militares, narcotr√°fico, robos ‚Üí SEGURIDAD Y DEFENSA hasta completar 4

3. IMPORTANTE:
   - Devolv√© SOLO las URLs, no t√≠tulos ni descripciones
   - Las URLs deben estar completas (https://...)
   - SIEMPRE 4 URLs por categor√≠a (excepto PRINCIPALES que son 5)
   - Si una categor√≠a parece tener pocas noticias, busc√° m√°s profundo en la lista
   - OBJETIVO: 9 categor√≠as √ó 4 art√≠culos = 36 art√≠culos totales
   - Ejemplo deportes: F1 + f√∫tbol + tenis + b√°squet = 4 noticias
   - Ejemplo seguridad: crimen + narcotr√°fico + ataques + robos = 4 noticias

Devolv√© SOLO el objeto JSON con las URLs categorizadas.`

// Select and categorize articles with AI
export async function selectArticles(rawContent) {
  try {
    logger.info('ü§ñ Selecting articles with OpenAI...')

    const response = await getOpenAI().chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: SYSTEM_PROMPT },
        { role: 'user', content: rawContent },
      ],
      response_format: { type: 'json_object' },
      temperature: 0.8, // Increased to be more creative in selection
      max_tokens: 4000, // Increased for more articles
    })

    const selection = response.choices[0].message.content

    // Validate that it's valid JSON
    try {
      JSON.parse(selection)
      logger.info(
        `‚úÖ Article selection generated (${selection.length} characters)`
      )
    } catch (parseError) {
      logger.error('‚ùå Invalid JSON from OpenAI:', selection.substring(0, 200))
      throw new Error('OpenAI returned invalid JSON for article selection')
    }

    return selection
  } catch (error) {
    logger.error('Error selecting articles:', error)
    throw error
  }
}

// Process article: generate optimized title, description, and explanation in ONE call
export async function processArticle(title, content) {
  try {
    const PROCESS_PROMPT = `Sos un periodista argentino que procesa noticias para un resumen informativo.

Recib√≠s el t√≠tulo original y el contenido de una noticia. Deb√©s devolver SOLO un JSON con:
1. "title": t√≠tulo optimizado (m√°x 80 chars, claro, sin clickbait, sin nombre del portal)
2. "description": resumen corto de 1-2 l√≠neas (qu√© pas√≥, qui√©n, cu√°ndo) - m√°x 150 chars
3. "explained": explicaci√≥n informal para que cualquiera entienda la noticia

PARA "explained" - TONO Y ESTILO:
- Profesional pero cercano (NO uses "che", "boludo" ni exceso de lunfardo)
- Manten√© neutralidad pol√≠tica absoluta
- Pod√©s usar palabras coloquiales como "guita", "quilombo" si ayudan a clarificar

PARA "explained" - FORMATO VISUAL (MUY IMPORTANTE):
- PROHIBIDO usar markdown: NO uses #, ##, ###, **, __, etc.
- Los t√≠tulos/subt√≠tulos se hacen SOLO con emojis + texto plano
- Ejemplo correcto: "üßâ ¬øQu√© dijo X, b√°sicamente?" o "üí∏ ¬øPor qu√© no juntaron reservas?"
- Us√° bullets con s√≠mbolos como: üëâ, ‚Ä¢, ‚úÖ, ‚ùå (NO uses - o * para bullets)
- Frases cortas y directas
- L√≠neas en blanco entre secciones para respirar

ESTRUCTURA del "explained":
- Divid√≠ en bloques tem√°ticos con preguntas como subt√≠tulos
- Cada secci√≥n empieza con emoji + pregunta o t√≠tulo descriptivo
- Us√° bullets para listar puntos clave
- Cerr√° con un resumen corto de lo m√°s importante

Devolv√© SOLO un JSON v√°lido con esta estructura:
{
  "title": "T√≠tulo optimizado",
  "description": "Resumen corto de la noticia",
  "explained": "Explicaci√≥n completa en texto plano con emojis"
}`

    const response = await getOpenAI().chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: PROCESS_PROMPT },
        {
          role: 'user',
          content: `T√≠tulo original: ${title}\n\nContenido:\n${content.substring(
            0,
            4000
          )}`,
        },
      ],
      temperature: 0.7,
      max_tokens: 2500,
      response_format: { type: 'json_object' },
    })

    const result = JSON.parse(response.choices[0].message.content)
    return {
      title: result.title || title, // Fallback to original
      description: result.description || 'Sin descripci√≥n disponible',
      explained: result.explained || 'Explicaci√≥n no disponible',
    }
  } catch (error) {
    logger.error('Error processing article:', error.message)
    // Return valid fallback to not break the flow
    return {
      title: title.substring(0, 80),
      description: 'Resumen no disponible temporalmente',
      explained: 'La explicaci√≥n no pudo ser generada en este momento.',
    }
  }
}

// DEPRECATED - Use processArticle() instead
export async function explainArticle(title, content) {
  try {
    const EXPLAIN_PROMPT = `Sos un periodista argentino que explica noticias de forma clara y accesible para cualquier persona.

Tu objetivo es explicar la noticia de manera que cualquiera la entienda, sin perder profesionalismo.

TONO Y ESTILO:
- Profesional pero cercano (NO uses "che", "boludo" ni exceso de lunfardo)
- Manten√© neutralidad pol√≠tica absoluta
- Pod√©s usar palabras coloquiales como "guita", "quilombo" si ayudan a clarificar
- Explic√° conceptos t√©cnicos/pol√≠ticos de forma simple, con ejemplos concretos

FORMATO VISUAL (MUY IMPORTANTE):
- PROHIBIDO usar markdown: NO uses #, ##, ###, **, __, etc.
- Los t√≠tulos/subt√≠tulos se hacen SOLO con emojis + texto plano
- Ejemplo correcto: "üßâ ¬øQu√© dijo X, b√°sicamente?" o "üí∏ ¬øPor qu√© no juntaron reservas?"
- Us√° bullets con s√≠mbolos por ejemplo estos: üëâ, ‚Ä¢, ‚úÖ, ‚ùå (NO uses - o * para bullets)
- Frases cortas y directas
- L√≠neas en blanco entre secciones para respirar

ESTRUCTURA:
- Divid√≠ en bloques tem√°ticos con preguntas como subt√≠tulos
- Cada secci√≥n empieza con emoji + pregunta o t√≠tulo descriptivo
- Us√° bullets para listar puntos clave
- Cerr√° con un resumen corto de lo m√°s importante

CONTENIDO:
- Explic√° los hechos principales de forma clara
- Traduc√≠/aclar√° entre par√©ntesis lo que sea complejo
- Ejemplos concretos cuando ayude

Devolv√© SOLO la explicaci√≥n en texto plano con emojis, sin markdown.`

    const response = await getOpenAI().chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: EXPLAIN_PROMPT },
        {
          role: 'user',
          content: `T√≠tulo: ${title}\n\nContenido:\n${content.substring(
            0,
            3000
          )}`, // Limit to avoid token excess
        },
      ],
      temperature: 0.8, // More creative for informal tone
      max_tokens: 1000,
    })

    const explanation = response.choices[0].message.content
    return explanation
  } catch (error) {
    logger.error('Error explaining article:', error)
    return null // If it fails, don't block everything
  }
}

// Extract URLs and categories from selection JSON
function extractSelections(selectionJson) {
  try {
    const data = JSON.parse(selectionJson)
    const selections = []

    data.categories.forEach((category) => {
      const categoryName = category.name

      // Include ALL categories, including PRINCIPALES
      // Filtering for UI is done in the frontend
      category.urls.forEach((url) => {
        selections.push({ url, category: categoryName })
      })
    })

    logger.info(
      `üîó Extracted ${selections.length} URLs from all categories (including PRINCIPALES)`
    )
    return selections
  } catch (error) {
    logger.error('Error parsing selection JSON:', error)
    return []
  }
}

// Check if the cache is recent (less than 30 minutes)
export async function isCacheRecent() {
  const cached = await NewsCache.findOne().sort({ createdAt: -1 })

  if (!cached) return false

  const cacheAge = Date.now() - new Date(cached.createdAt).getTime()
  const thirtyMinutes = 30 * 60 * 1000
  return cacheAge < thirtyMinutes
}

// Get the latest cached articles (for users)
export async function getSummary() {
  try {
    const cached = await NewsCache.findOne().sort({ createdAt: -1 })

    if (cached) {
      const cacheAge = Date.now() - new Date(cached.createdAt).getTime()
      const minutes = Math.floor(cacheAge / 60000)
      logger.info(`üì¶ Using cached articles (${minutes} min old)`)
      return cached.articles // Return array directly
    }

    // If no cache, generate one (only the first time)
    logger.info('‚ö†Ô∏è No cache found, generating first summary...')
    return await refreshSummary()
  } catch (error) {
    logger.error('Error getting summary:', error)
    throw new Error('No se pudo obtener resumen de noticias')
  }
}

// Refresh the cache (only for cron jobs)
export async function refreshSummary() {
  try {
    logger.info('üîÑ Refreshing news cache...')

    // STEP 1: Scrape fresh content from all portals
    const scrapedData = await scrapeAllPortals()

    // Collect all articles (title + URL only)
    const allArticles = []
    scrapedData.forEach((portalData) => {
      if (!portalData.error && portalData.articles) {
        allArticles.push(...portalData.articles)
      }
    })

    if (allArticles.length === 0) {
      throw new Error('No articles scraped from portals')
    }

    logger.info(`üì∞ Collected ${allArticles.length} articles`)

    // STEP 2: Format articles for AI selection (title + URL only)
    const articleList = allArticles
      .map((article) => `[${article.portal}] ${article.title}\n${article.url}`)
      .join('\n\n')

    // STEP 3: AI selects and categorizes important articles
    const selectionRaw = await selectArticles(articleList)

    // Parse selection
    let selections
    try {
      selections = extractSelections(selectionRaw)
      logger.info('‚úÖ Article selection parsed successfully')
    } catch (parseError) {
      logger.error('‚ùå Error parsing selection JSON:', parseError)
      logger.error('Raw selection:', selectionRaw.substring(0, 500))
      throw new Error('Selection is not valid JSON')
    }

    if (selections.length === 0) {
      throw new Error('No articles selected by AI')
    }

    // STEP 4: Scrape full content of selected articles
    logger.info('üì• Fetching full content for selected articles...')
    const scrapePromises = selections.map(({ url }) => scrapeArticle(url))
    const scrapedArticles = await Promise.all(scrapePromises)

    // STEP 5: Process each article with AI (generate title, description, explained) IN PARALLEL
    logger.info(
      `ü§ñ Processing ${scrapedArticles.length} articles in parallel...`
    )
    const processPromises = scrapedArticles.map(async (article, index) => {
      try {
        if (article.error || !article.content) {
          logger.warn(
            `‚ö†Ô∏è Skipping article due to scraping error: ${selections[index].url}`
          )
          return null
        }

        const portalName = new URL(article.url).hostname.replace('www.', '')
        const category = selections[index].category

        // Process article with AI (title + description + explained)
        logger.info(`ü§ñ Processing: ${article.title.substring(0, 50)}...`)
        const processed = await processArticle(article.title, article.content)

        return {
          category,
          title: processed.title,
          description: processed.description,
          url: article.url,
          content: article.content,
          portal: portalName,
          explained: processed.explained,
        }
      } catch (error) {
        logger.error(
          `‚ùå Error processing article ${selections[index].url}:`,
          error.message
        )
        return null
      }
    })

    const results = await Promise.all(processPromises)
    const fullArticles = results.filter((article) => article !== null)

    logger.info(
      `‚úÖ Processed ${fullArticles.length}/${selections.length} articles`
    )

    // Verify that we have at least some articles
    if (fullArticles.length === 0) {
      throw new Error('No articles were successfully processed')
    }

    // STEP 6: Save to cache (articles array only)
    await NewsCache.create({
      summary: '', // Deprecated, kept for schema
      articles: fullArticles,
      rawContent: '', // Deprecated
    })

    // Clean old cache (keep only the last 10)
    const allCache = await NewsCache.find().sort({ createdAt: -1 }).skip(10)

    if (allCache.length > 0) {
      await NewsCache.deleteMany({
        _id: { $in: allCache.map((c) => c._id) },
      })
      logger.info(`üóëÔ∏è Cleaned ${allCache.length} old cache entries`)
    }

    logger.info('‚úÖ News cache refreshed successfully')
    return fullArticles // Return array directly
  } catch (error) {
    logger.error('Error refreshing summary:', error.message || error)
    logger.error('Error stack:', error.stack)

    // Return last cache as fallback
    const lastCache = await NewsCache.findOne().sort({ createdAt: -1 })

    if (lastCache && lastCache.articles && lastCache.articles.length > 0) {
      logger.info('‚ö†Ô∏è Returning last cached articles due to error')
      return lastCache.articles
    }

    throw new Error(
      'No se pudo refrescar el resumen de noticias: ' + (error.message || error)
    )
  }
}
